# -*- coding: utf-8 -*-
"""part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ECsZBprn01vNVANpxkAGi-dU6bhBO57Q

## Yacht Hydrodynamics Data Set
<br>

**About the Dataset**

1. Prediction of residuary resistance of sailing yachts at the initial design stage is of a great value for evaluating the ship's performance and for estimating the required propulsive power. 

2. Essential inputs include the basic hull dimensions and the boat velocity. 

3. The Delft data set comprises 308 full-scale experiments, which were performed at the Delft Ship Hydromechanics Laboratory for that purpose. 


<br>

**Attribute Information**

*Variations concern hull geometry coefficients and the Froude number:*

1. Longitudinal position of the center of buoyancy, adimensional.
2. Prismatic coefficient, adimensional.
3. Length-displacement ratio, adimensional.
4. Beam-draught ratio, adimensional.
5. Length-beam ratio, adimensional.
6. Froude number, adimensional.

*The measured variable is the residuary resistance per unit weight of displacement:*

7. Residuary resistance per unit weight of displacement, adimensional.

<br>
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data",
                 delimiter=" ",
                 names=["lng_pos","pris_coeff","ld_ratio","bd_ratio","lb_ratio","froude","resistance"],
                 index_col=False)

df["pris_coeff"][df.pris_coeff.isnull() == True].index

df1 = df.copy()
df1.pris_coeff.fillna(method="bfill", inplace=True)

plt.figure(figsize=(12,8))
sns.heatmap(df1.corr(), annot=True, cmap="coolwarm")
plt.draw()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Splitting data

X = df1.drop("resistance", axis=1).values

y = df1.resistance.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Feature Scaling

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""
Function to compute the value of RSS
"""

def RSS(X,Y,m):
    temp=0
    for i in range(X[0].shape[0]):
        predicted_value=(m[0] + (m[1]*X[0][i]) + (m[2]*X[1][i]) + (m[3]*X[2][i]) + (m[4]*X[3][i]) + (m[5]*X[4][i]) + (m[6]*X[5][i]))
        actual_value=Y[i]
        temp = temp+ ((predicted_value-actual_value)**2)
    return (np.sqrt(temp/X[0].shape[0]))


def predictedValueForSpecificRow(X,i,m):
    return (m[0] + (m[1]*X[0][i]) + (m[2]*X[1][i]) + (m[3]*X[2][i]) + (m[4]*X[3][i]) + (m[5]*X[4][i]) + (m[6]*X[5][i]))


"""

Gradient Descent Algorithm implementation for our dataset
"""

def gradientDescentAlgorithm(X,Y,learning_rate,maximum_iterations):
    print("Training MLR model using Gradient Descent")

    # maximum_iterations=5000

    has_converged=False
    print(X[0].shape[0])
    rows_count= X[0].shape[0]

    m=[0,0,0,0,0,0,0]

    error= RSS(X,Y,m)
    print("Initial value of RSS(Cost Function):", error)

    i=0
    
    g=[0,0,0,0,0,0,0]
    while not has_converged:
      for index2 in range(0,7):
        if index2==0:
          g[index2]=(1.0/rows_count) * (sum([(predictedValueForSpecificRow(X,i,m)-Y[i]) for i in range(rows_count)]))
        else:
          g[index2]=(1.0/rows_count) * (sum([(predictedValueForSpecificRow(X,i,m)-Y[i])* X[index2-1][i] for i in range(rows_count)]))

      for index1 in range(0,7):
        m[index1]=m[index1]-learning_rate*g[index1]       

        current_error= RSS(X,Y,m)

        if i % 1000==0:
            print("Iteration", i+1, "Current value of RSS based on updated value of model parameters", current_error)
        error=current_error
        i=i+1

        if i==maximum_iterations:
            print("Training Process Halted as number of iterations maxed out ")
            has_converged=True
    return m

"""
Making predictions
"""

def predict(coefficients, X):
    m0=coefficients[0]
    m1=coefficients[1]
    m2=coefficients[2]
    m3=coefficients[3]
    m4=coefficients[4]
    m5=coefficients[5]
    m6=coefficients[6]

    predictions=[]

    # Check whether the X given to us is a list of multiple values

    if len(X)>1: #[[]]
        for x in X:
            predictions.append(m0 + (m1*x[0]) + (m2*x[1]) + (m3*x[2]) + (m4*x[3]) + (m5*x[4]) + (m6*x[5]) )
        return predictions

    x=X[0]
    Y=   m0 + (m1*x[0]) + (m2*x[1]) + (m3*x[2]) + (m4*x[3]) + (m5*x[4]) + (m6*x[5])  
    return Y

#For Training 
dummy_x = X_train.T.ravel()
dummy_y = y_train.T.ravel()

X_train_new = []
dy = len(dummy_y)
for i in range(len(df.columns)-1):
  X_train_new.append(np.array(dummy_x[i*dy:(i+1)*dy]))

y_train_new = [y_train]

#For Testing  
dummy_x_test = X_test.T.ravel()
dummy_y_test = y_test.T.ravel()

X_test_new = []
dy_test = len(dummy_y_test)
for i in range(len(df.columns)-1):
  X_test_new.append(np.array(dummy_x_test[i*dy_test:(i+1)*dy_test]))

y_test_new = [y_test]

# FINE TUNING OF MODEL FOR TRAINING DATA

rr = []
errorr = []
param_grid = []

for l_rate in np.arange(0.001, 0.05, 0.01):
  for max_iterations in np.arange(5000,8000,1000):

    print()
    param_grid.append(tuple([l_rate, max_iterations]))
    print(l_rate,max_iterations)
    coefficients = gradientDescentAlgorithm(X_train_new,y_train_new[0],learning_rate=l_rate,maximum_iterations=max_iterations)
    X=np.transpose(X_train_new)
    Y=y_train_new[0]
    rss=0

    for index in range(0,len(X)):
      actual_value= Y[index]
      predicted_value=predict(coefficients, [X[index,0:6]])
      rss=rss+pow((actual_value-predicted_value),2)

    RSE=np.sqrt((1/float(X_test.shape[0] -2))*rss)
      
    print("Residual Standard Error:",RSE)

    print("Residual Standard Error (Over the average interval):",(RSE)/np.mean(Y)*100)

    #Calculation of R_Squared metric

    TSS=0

    for index in range(0,len(X)):
      actual_value=Y[index]
      TSS=TSS+((actual_value-np.mean(Y))**2)
      R_Squared=(TSS-rss)/TSS

    print("The value for R_Squared:", R_Squared)
    rr.append(R_Squared)
    errorr.append(RSE)

    print(rr)
    print(errorr)

# FINE TUNING OF MODEL FOR TRAINING and TESTING DATA

rr = []
errorr = []
rr_test = []
errorr_test = []
param_grid = []

learning_rate_alpha = np.arange(0.001, 0.05, 0.01)
maximum_iter = np.arange(5000,15000,1000)

for l_rate in learning_rate_alpha:
  for max_iterations in maximum_iter:

    print()
    param_grid.append(tuple([l_rate, max_iterations]))
    print(l_rate,max_iterations)
    coefficients = gradientDescentAlgorithm(X_train_new,y_train_new[0],learning_rate=l_rate,maximum_iterations=max_iterations)


    ## training data
    print("Train Data :")
    X=np.transpose(X_train_new)
    Y=y_train_new[0]
    rss=0

    for index in range(0,len(X)):
      actual_value= Y[index]
      predicted_value=predict(coefficients, [X[index,0:6]])
      rss=rss+pow((actual_value-predicted_value),2)

    RSE=np.sqrt((1/float(X_test.shape[0] -2))*rss)
      
    print("Residual Standard Error:",RSE)

    print("Residual Standard Error (Over the average interval):",(RSE)/np.mean(Y)*100)

    #Calculation of R_Squared metric for training data

    TSS=0

    for index in range(0,len(X)):
      actual_value=Y[index]
      TSS=TSS+((actual_value-np.mean(Y))**2)
      R_Squared=(TSS-rss)/TSS

    print("The value for R_Squared:", R_Squared)
    rr.append(R_Squared)
    errorr.append(RSE)

    ## test data
    print("Test Data :")
    X_test=np.transpose(X_test_new)
    Y_test=y_test_new[0]
    rss_t=0

    for index in range(0,len(X_test)):
        actual_value= Y_test[index]
        predicted_value=predict(coefficients, [X_test[index,0:6]])
        rss_t=rss_t+((actual_value-predicted_value)**2)

    RSE_test=np.sqrt((1/float(X_test.shape[0] -2))*rss_t)

    print("Residual Standard Error:",RSE_test)

    print("Residual Standard Error (Over the average interval):",(RSE_test)/np.mean(Y_test)*100)

    #Calculation of R_Squared metric for testing data
    TSS_test=0

    for index in range(0,len(X_test)):
        actual_value=Y_test[index]
        TSS_test=TSS_test+((actual_value-np.mean(Y_test))**2)
    R_Squared_test=(TSS_test-rss_t)/TSS_test

    print("The value for R_Squared:", R_Squared_test)	  
    rr_test.append(R_Squared_test)
    errorr_test.append(RSE_test)

# plotting model score for training and testing data

plt.figure(figsize=(12,8))

ltot = len(learning_rate_alpha) * len(maximum_iter)

plt.plot(np.arange(ltot), rr, color="red", label="test_data")
plt.plot(np.arange(ltot), rr_test, color="blue", label="test_data")

plt.title("Linear Regression Using Gradient Descent")

plt.xticks(np.arange(ltot), labels = param_grid, rotation=90)
plt.xlabel("Learning_rate / Max_iterations")
plt.ylabel("Score")
plt.legend(loc="best")
plt.tight_layout()
plt.draw()
